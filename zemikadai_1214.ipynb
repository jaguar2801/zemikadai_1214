{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#データの読み込み\n",
    "with open('pg2265.txt', 'r', encoding='utf-8') as f: \n",
    "    text=f.read()\n",
    "\n",
    "text = text[15858:]\n",
    "#テキストデータで使われている文字を抽出\n",
    "chars = set(text)\n",
    "#文字を整数に変更\n",
    "char2int = {ch:i for i,ch in enumerate(chars)}\n",
    "#整数を文字に変更\n",
    "int2char = dict(enumerate(chars))\n",
    "\n",
    "#char2intを元にテキストデータを整数に変更\n",
    "text_ints = np.array([char2int[ch] for ch in text], \n",
    "                     dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 2540)\n",
      "(64, 2540)\n",
      "[ 1 27 18 47  1 52 15 36 18 30]\n",
      "[27 18 47  1 52 15 36 18 30 61]\n",
      "The Tragedie of Hamlet\n",
      "\n",
      "Actus Primus. Scoena Prima\n"
     ]
    }
   ],
   "source": [
    "def reshape_data(sequence, batch_size, num_steps):\n",
    "    mini_batch_length = batch_size * num_steps\n",
    "    num_batches = int(len(sequence) / mini_batch_length)\n",
    "    if num_batches*mini_batch_length + 1 > len(sequence):\n",
    "        num_batches = num_batches - 1\n",
    "    \n",
    "    #シーケンスの最後の部分から完全なバッチにならない半端な文字を削除\n",
    "    x = sequence[0 : num_batches*mini_batch_length]\n",
    "    y = sequence[1 : num_batches*mini_batch_length + 1]\n",
    "    #xとyをシーケンスのバッチのリストに分割\n",
    "    x_batch_splits = np.split(x, batch_size)\n",
    "    y_batch_splits = np.split(y, batch_size)\n",
    "    #それらのバッチを結合\n",
    "    x = np.stack(x_batch_splits)\n",
    "    y = np.stack(y_batch_splits)\n",
    "    \n",
    "    return x, y\n",
    "\n",
    "\n",
    "train_x, train_y = reshape_data(text_ints, 64, 10)\n",
    "print(train_x.shape)\n",
    "print(train_y.shape)\n",
    "print(train_x[0, :10])\n",
    "print(train_y[0, :10])\n",
    "print(''.join(int2char[i] for i in train_x[0, :50]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 15) (64, 15)  The Tragedie of      he Tragedie of \n",
      "(64, 15) (64, 15)   Hamlet**Actus       Hamlet**Actus P\n",
      "(64, 15) (64, 15)  Primus. Scoena       rimus. Scoena P\n",
      "(64, 15) (64, 15)  Prima.**Enter B      rima.**Enter Ba\n",
      "(64, 15) (64, 15)  arnardo and Fra      rnardo and Fran\n",
      "(64, 15) (64, 15)  ncisco two Cent      cisco two Centi\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(123)\n",
    "#ミニバッチを順番に処理をしていくためのコード\n",
    "def create_batch_generator(data_x, data_y, num_steps):\n",
    "    batch_size, tot_batch_length = data_x.shape    \n",
    "    num_batches = int(tot_batch_length/num_steps)\n",
    "    for b in range(num_batches):\n",
    "        yield (data_x[:, b*num_steps: (b+1)*num_steps], \n",
    "               data_y[:, b*num_steps: (b+1)*num_steps])\n",
    "        \n",
    "bgen = create_batch_generator(train_x[:,:100], train_y[:,:100], 15)\n",
    "for b in bgen:\n",
    "    print(b[0].shape, b[1].shape, end='  ')\n",
    "    print(''.join(int2char[i] for i in b[0][0,:]).replace('\\n', '*'), '    ',\n",
    "          ''.join(int2char[i] for i in b[1][0,:]).replace('\\n', '*'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ・コンストラクタ\n",
    "   ##  学習パラメータを設定し、計算グラフを作成する。さらに、トレーニングモードとサンプリングモードに基づいて計算グラフを作成する\n",
    "# ・buildメソッド\n",
    "   ##    データを供給するためのプレースホルダを定義し、LSTMセルを使ってRNNを作成する。\n",
    "# ・trainメソッド\n",
    "   ##   ミニバッチを順番に処理しながら、指定された数のエポックでRNNのトレーニングを行う。\n",
    "# ・sampleメソッド\n",
    "   ##   与えられた文字列を元に文字列を作成  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "class CharRNN(object):\n",
    "    #コンストラクタ\n",
    "    def __init__(self, num_classes, batch_size=64, \n",
    "                 num_steps=100, lstm_size=128, \n",
    "                 num_layers=1, learning_rate=0.001, \n",
    "                 keep_prob=0.5, grad_clip=5, \n",
    "                 sampling=False):\n",
    "        self.num_classes = num_classes\n",
    "        self.batch_size = batch_size\n",
    "        self.num_steps = num_steps\n",
    "        self.lstm_size = lstm_size\n",
    "        self.num_layers = num_layers\n",
    "        self.learning_rate = learning_rate\n",
    "        self.keep_prob = keep_prob\n",
    "        self.grad_clip = grad_clip\n",
    "        \n",
    "        self.g = tf.Graph()\n",
    "        with self.g.as_default():\n",
    "            tf.set_random_seed(123)\n",
    "\n",
    "            self.build(sampling=sampling)\n",
    "            self.saver = tf.train.Saver()\n",
    "            self.init_op = tf.global_variables_initializer()\n",
    "     \n",
    "    #buildメソッド\n",
    "    def build(self, sampling):\n",
    "        if sampling == True:\n",
    "            batch_size, num_steps = 1, 1\n",
    "        else:\n",
    "            batch_size = self.batch_size\n",
    "            num_steps = self.num_steps\n",
    "\n",
    "        tf_x = tf.placeholder(tf.int32, \n",
    "                              shape=[batch_size, num_steps], \n",
    "                              name='tf_x')\n",
    "        tf_y = tf.placeholder(tf.int32, \n",
    "                              shape=[batch_size, num_steps], \n",
    "                              name='tf_y')\n",
    "        tf_keepprob = tf.placeholder(tf.float32, \n",
    "                              name='tf_keepprob')\n",
    "\n",
    "        # One-hot エンコーディングを適用\n",
    "        x_onehot = tf.one_hot(tf_x, depth=self.num_classes)\n",
    "        y_onehot = tf.one_hot(tf_y, depth=self.num_classes)\n",
    "\n",
    "        #多層RNNのセルを構築\n",
    "        cells = tf.contrib.rnn.MultiRNNCell(\n",
    "            [tf.contrib.rnn.DropoutWrapper(\n",
    "                tf.contrib.rnn.BasicLSTMCell(self.lstm_size), \n",
    "                output_keep_prob=tf_keepprob) \n",
    "            for _ in range(self.num_layers)])\n",
    "        \n",
    "        #初期状態を定義\n",
    "        self.initial_state = cells.zero_state(\n",
    "                    batch_size, tf.float32)\n",
    "\n",
    "        #RNNで各シーケンスステップを実行\n",
    "        lstm_outputs, self.final_state = tf.nn.dynamic_rnn(\n",
    "                    cells, x_onehot, \n",
    "                    initial_state=self.initial_state)\n",
    "        \n",
    "        print('  << lstm_outputs  >>', lstm_outputs)\n",
    "　　　　#２次元テンソルに変形　\n",
    "        seq_output_reshaped = tf.reshape(\n",
    "                    lstm_outputs, \n",
    "                    shape=[-1, self.lstm_size],\n",
    "                    name='seq_output_reshaped')\n",
    "　　　　#総入力を取得　\n",
    "        logits = tf.layers.dense(\n",
    "                    inputs=seq_output_reshaped, \n",
    "                    units=self.num_classes,\n",
    "                    activation=None,\n",
    "                    name='logits')\n",
    "　　　　#次の文字バッチの確率を計算\n",
    "        proba = tf.nn.softmax(\n",
    "                    logits, \n",
    "                    name='probabilities')\n",
    "        print(proba)\n",
    "　　　　#コスト関数を定義　\n",
    "        y_reshaped = tf.reshape(\n",
    "                    y_onehot, \n",
    "                    shape=[-1, self.num_classes],\n",
    "                    name='y_reshaped')\n",
    "        cost = tf.reduce_mean(\n",
    "                    tf.nn.softmax_cross_entropy_with_logits(\n",
    "                        logits=logits, \n",
    "                        labels=y_reshaped),\n",
    "                    name='cost')\n",
    "\n",
    "        # 勾配発散問題を回避するための勾配刈り込み\n",
    "        tvars = tf.trainable_variables()\n",
    "        grads, _ = tf.clip_by_global_norm(\n",
    "                    tf.gradients(cost, tvars), \n",
    "                    self.grad_clip)\n",
    "        optimizer = tf.train.AdamOptimizer(self.learning_rate)\n",
    "        train_op = optimizer.apply_gradients(\n",
    "                    zip(grads, tvars),\n",
    "                    name='train_op')\n",
    "    #trainメソッド    \n",
    "    def train(self, train_x, train_y, \n",
    "              num_epochs, ckpt_dir='./model/'):\n",
    "        #チェックポイントディレクトリがまだ存在しない場合は作成\n",
    "        if not os.path.exists(ckpt_dir):\n",
    "            os.mkdir(ckpt_dir)\n",
    "            \n",
    "        with tf.Session(graph=self.g) as sess:\n",
    "            sess.run(self.init_op)\n",
    "\n",
    "            n_batches = int(train_x.shape[1]/self.num_steps)\n",
    "            iterations = n_batches * num_epochs\n",
    "            for epoch in range(num_epochs):\n",
    "\n",
    "                # ネットワークをトレーニング\n",
    "                new_state = sess.run(self.initial_state)\n",
    "                loss = 0\n",
    "                #ミニバッチジェネレータ\n",
    "                bgen = create_batch_generator(\n",
    "                        train_x, train_y, self.num_steps)\n",
    "                for b, (batch_x, batch_y) in enumerate(bgen, 1):\n",
    "                    iteration = epoch*n_batches + b\n",
    "                    \n",
    "                    feed = {'tf_x:0': batch_x,\n",
    "                            'tf_y:0': batch_y,\n",
    "                            'tf_keepprob:0': self.keep_prob,\n",
    "                            self.initial_state : new_state}\n",
    "                    batch_cost, _, new_state = sess.run(\n",
    "                            ['cost:0', 'train_op', \n",
    "                                self.final_state],\n",
    "                            feed_dict=feed)\n",
    "                    if iteration % 10 == 0:\n",
    "                        print('Epoch %d/%d Iteration %d'\n",
    "                              '| Training loss: %.4f' % (\n",
    "                              epoch + 1, num_epochs, \n",
    "                              iteration, batch_cost))\n",
    "\n",
    "                #トレーニングモデルを保存\n",
    "                self.saver.save(\n",
    "                        sess, os.path.join(\n",
    "                            ckpt_dir, 'language_modeling.ckpt'))\n",
    "                              \n",
    "                              \n",
    "    #　sampleメソッド            \n",
    "    def sample(self, output_length, \n",
    "               ckpt_dir, starter_seq=\"The \"):\n",
    "        observed_seq = [ch for ch in starter_seq]        \n",
    "        with tf.Session(graph=self.g) as sess:\n",
    "            self.saver.restore(\n",
    "                sess, \n",
    "                tf.train.latest_checkpoint(ckpt_dir))\n",
    "            #　starter_seqを使ってモデルを実行\n",
    "            new_state = sess.run(self.initial_state)\n",
    "            for ch in starter_seq:\n",
    "                x = np.zeros((1, 1))\n",
    "                x[0,0] = char2int[ch]\n",
    "                feed = {'tf_x:0': x,\n",
    "                        'tf_keepprob:0': 1.0,\n",
    "                        self.initial_state: new_state}\n",
    "                proba, new_state = sess.run(\n",
    "                        ['probabilities:0', self.final_state], \n",
    "                        feed_dict=feed)\n",
    "\n",
    "            ch_id = get_top_char(proba, len(chars))\n",
    "            observed_seq.append(int2char[ch_id])\n",
    "            \n",
    "            #　更新されたobserved_seqを使ってモデルを実行\n",
    "            for i in range(output_length):\n",
    "                x[0,0] = ch_id\n",
    "                feed = {'tf_x:0': x,\n",
    "                        'tf_keepprob:0': 1.0,\n",
    "                        self.initial_state: new_state}\n",
    "                proba, new_state = sess.run(\n",
    "                        ['probabilities:0', self.final_state], \n",
    "                        feed_dict=feed)\n",
    "\n",
    "                ch_id = get_top_char(proba, len(chars))\n",
    "                observed_seq.append(int2char[ch_id])\n",
    "\n",
    "        return ''.join(observed_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_char(probas, char_size, top_n=5):\n",
    "    p = np.squeeze(probas)\n",
    "    p[np.argsort(p)[:-top_n]] = 0.0\n",
    "    p = p / np.sum(p)\n",
    "    ch_id = np.random.choice(char_size, 1, p=p)[0]\n",
    "    return ch_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CharRNNモデルの作成とトレーニング"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute 'contrib'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-5dd9d0a3428a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m                                 num_steps)\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mrnn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCharRNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m rnn.train(train_x, train_y, \n\u001b[1;32m      9\u001b[0m           \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-22-ed398b60834f>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, num_classes, batch_size, num_steps, lstm_size, num_layers, learning_rate, keep_prob, grad_clip, sampling)\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_random_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m123\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msampling\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msampling\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSaver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_variables_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-22-ed398b60834f>\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self, sampling)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;31m### Build the multi-layer RNN cells\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         cells = tf.contrib.rnn.MultiRNNCell(\n\u001b[0m\u001b[1;32m     49\u001b[0m             [tf.contrib.rnn.DropoutWrapper(\n\u001b[1;32m     50\u001b[0m                 \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBasicLSTMCell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'contrib'"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "num_steps = 100 \n",
    "train_x, train_y = reshape_data(text_ints, \n",
    "                                batch_size, \n",
    "                                num_steps)\n",
    "\n",
    "rnn = CharRNN(num_classes=len(chars), batch_size=batch_size)\n",
    "rnn.train(train_x, train_y, \n",
    "          num_epochs=100,\n",
    "          ckpt_dir='./model-100/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## サンプリングモードのCharRNNモデル"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  << lstm_outputs  >> Tensor(\"rnn/transpose_1:0\", shape=(1, 1, 128), dtype=float32)\n",
      "Tensor(\"probabilities:0\", shape=(1, 65), dtype=float32)\n",
      "INFO:tensorflow:Restoring parameters from ./model-200/language_modeling.ckpt\n",
      "The dast,\n",
      "With a with a peote sintist and theng theres\n",
      "To be to me to that theng in a mattes ast the\n",
      "\n",
      "   Ham. Where he somier doue as these make seaker\n",
      "To the Pattangonst they out his wilce,\n",
      "To sere the Songut is wordd bode arate\n",
      "\n",
      "   Ham. Houst\n",
      "\n",
      "   Hor. The serull hus shall broone on him\n",
      "\n",
      "   Ophe. Woo then  of the bost to hounds, we tound and thend it,\n",
      "Wish whore thought to that that that shinge and Madestiess\n",
      "\n",
      "   Ham. What meane since the mathirg alleste in that, wall not\n",
      "Whole warks and aruale, and\n"
     ]
    }
   ],
   "source": [
    "del rnn\n",
    "\n",
    "np.random.seed(123)\n",
    "rnn = CharRNN(len(chars), sampling=True)\n",
    "print(rnn.sample(ckpt_dir='./model-200/', \n",
    "                 output_length=500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
